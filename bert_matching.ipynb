{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c64bed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nishmaanair/Desktop/ai_recruitment_tool/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded.\n",
      "Using cleaned data from: ./data/cleaned_resume_job_data.csv\n",
      "Using Sentence-BERT model: all-MiniLM-L6-v2\n",
      "Output will be saved to: ./data/resume_job_match_with_similarity.csv\n"
     ]
    }
   ],
   "source": [
    "#Import Libraries and Configuration\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the cleaned data CSV generated by the previous script\n",
    "CLEANED_DATA_PATH = \"./data/cleaned_resume_job_data.csv\"\n",
    "\n",
    "# The Sentence-BERT model to use. 'all-MiniLM-L6-v2' is a good general-purpose model.\n",
    "# You can explore others like 'all-mpnet-base-v2' for higher performance (but slower).\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "\n",
    "# Path to save the DataFrame with the new similarity scores\n",
    "OUTPUT_CSV_PATH = \"./data/resume_job_match_with_similarity.csv\"\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"Using cleaned data from: {CLEANED_DATA_PATH}\")\n",
    "print(f\"Using Sentence-BERT model: {MODEL_NAME}\")\n",
    "print(f\"Output will be saved to: {OUTPUT_CSV_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9ab3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cleaned data from ./data/cleaned_resume_job_data.csv. Shape: (10000, 5)\n",
      "\n",
      "Sample of cleaned data (first 5 rows):\n",
      "                             job_description_cleaned  \\\n",
      "0  data analyst needed with experience in sql, ex...   \n",
      "1  data scientist needed with experience in stati...   \n",
      "2  software engineer needed with experience in sy...   \n",
      "3  ml engineer needed with experience in python, ...   \n",
      "4  software engineer needed with experience in re...   \n",
      "\n",
      "                                      resume_cleaned  match_score  \n",
      "0  experienced professional skilled in sql, power...            4  \n",
      "1  experienced professional skilled in python, de...            4  \n",
      "2  experienced professional skilled in wait, git,...            5  \n",
      "3  experienced professional skilled in return, de...            4  \n",
      "4  experienced professional skilled in rest apis,...            5  \n"
     ]
    }
   ],
   "source": [
    "# Check if the cleaned data file exists\n",
    "if not os.path.exists(CLEANED_DATA_PATH):\n",
    "    print(f\"Error: Cleaned data file not found at '{CLEANED_DATA_PATH}'.\")\n",
    "    print(\"Please ensure the previous data preparation script ran successfully and saved the file.\")\n",
    "    # You might want to add a sys.exit() here if this is critical for execution\n",
    "    # For a notebook, we'll just print an error and stop.\n",
    "else:\n",
    "    try:\n",
    "        df = pd.read_csv(CLEANED_DATA_PATH)\n",
    "        print(f\"Loaded cleaned data from {CLEANED_DATA_PATH}. Shape: {df.shape}\")\n",
    "        print(\"\\nSample of cleaned data (first 5 rows):\")\n",
    "        print(df[['job_description_cleaned', 'resume_cleaned', 'match_score']].head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cleaned data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de659ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence-BERT model: all-MiniLM-L6-v2...\n",
      "Model loaded successfully.\n",
      "GPU not available, using CPU.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Pre-trained Sentence-BERT Model\n",
    "\n",
    "print(f\"Loading Sentence-BERT model: {MODEL_NAME}...\")\n",
    "try:\n",
    "    # This downloads the model if it's not already cached locally\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    print(\"Model loaded successfully.\")\n",
    "    # Check if GPU is available and use it\n",
    "    if torch.cuda.is_available():\n",
    "        model.to(torch.device(\"cuda\"))\n",
    "        print(\"Model moved to GPU for faster processing.\")\n",
    "    else:\n",
    "        print(\"GPU not available, using CPU.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Sentence-BERT model. Make sure you have 'sentence-transformers' and 'torch' installed (`pip install sentence-transformers torch`). Error: {e}\")\n",
    "    print(\"If you encounter issues, try running `pip install --upgrade transformers sentence-transformers torch`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63a25361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for job descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 313/313 [00:10<00:00, 31.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating embeddings for resumes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 313/313 [00:07<00:00, 43.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 10000 job embeddings.\n",
      "Generated 10000 resume embeddings.\n",
      "Each embedding has a dimension of: 384\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Generate Embeddings for Job Descriptions and Resumes\n",
    "\n",
    "print(\"\\nGenerating embeddings for job descriptions...\")\n",
    "# Encode job descriptions into dense vector embeddings\n",
    "# convert_to_tensor=True ensures the output is a PyTorch tensor, useful for GPU acceleration\n",
    "# show_progress_bar=True provides visual feedback during encoding\n",
    "job_embeddings = model.encode(df['job_description_cleaned'].tolist(),\n",
    "                              convert_to_tensor=True,\n",
    "                              show_progress_bar=True)\n",
    "\n",
    "print(\"\\nGenerating embeddings for resumes...\")\n",
    "# Encode resumes into dense vector embeddings\n",
    "resume_embeddings = model.encode(df['resume_cleaned'].tolist(),\n",
    "                                 convert_to_tensor=True,\n",
    "                                 show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nGenerated {len(job_embeddings)} job embeddings.\")\n",
    "print(f\"Generated {len(resume_embeddings)} resume embeddings.\")\n",
    "print(f\"Each embedding has a dimension of: {job_embeddings.shape[1]}\") # Should be 384 for MiniLM-L6-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87161f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating cosine similarity between job descriptions and resumes...\n",
      "\n",
      "Sample of data with new BERT similarity scores (first 5 rows):\n",
      "                             job_description_cleaned  \\\n",
      "0  data analyst needed with experience in sql, ex...   \n",
      "1  data scientist needed with experience in stati...   \n",
      "2  software engineer needed with experience in sy...   \n",
      "3  ml engineer needed with experience in python, ...   \n",
      "4  software engineer needed with experience in re...   \n",
      "\n",
      "                                      resume_cleaned  match_score  \\\n",
      "0  experienced professional skilled in sql, power...            4   \n",
      "1  experienced professional skilled in python, de...            4   \n",
      "2  experienced professional skilled in wait, git,...            5   \n",
      "3  experienced professional skilled in return, de...            4   \n",
      "4  experienced professional skilled in rest apis,...            5   \n",
      "\n",
      "   bert_similarity_score  \n",
      "0               0.652394  \n",
      "1               0.372719  \n",
      "2               0.447902  \n",
      "3               0.516451  \n",
      "4               0.451312  \n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Calculate Cosine Similarity\n",
    "\n",
    "print(\"\\nCalculating cosine similarity between job descriptions and resumes...\")\n",
    "\n",
    "# util.cos_sim calculates the cosine similarity between two sets of embeddings.\n",
    "# It returns a matrix where element [i][j] is the similarity between embedding i from the first set\n",
    "# and embedding j from the second set.\n",
    "# Since we want the similarity for each (job_description, resume) pair, we need the diagonal elements.\n",
    "cosine_scores = util.cos_sim(job_embeddings, resume_embeddings).diag()\n",
    "\n",
    "# Convert the PyTorch tensor to a NumPy array and then to a Python list\n",
    "# .cpu() moves the tensor to CPU if it was on GPU, .numpy() converts to NumPy array\n",
    "df['bert_similarity_score'] = cosine_scores.cpu().numpy().tolist()\n",
    "\n",
    "print(\"\\nSample of data with new BERT similarity scores (first 5 rows):\")\n",
    "print(df[['job_description_cleaned', 'resume_cleaned', 'match_score', 'bert_similarity_score']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a023b767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving DataFrame with BERT similarity scores to: ./data/resume_job_match_with_similarity.csv\n",
      "DataFrame saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save Results\n",
    "\n",
    "print(f\"\\nSaving DataFrame with BERT similarity scores to: {OUTPUT_CSV_PATH}\")\n",
    "try:\n",
    "    df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "    print(\"DataFrame saved successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving data to CSV: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
